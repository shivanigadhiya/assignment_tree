{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What is a Projection, and How is it Used in PCA?\n",
    "A projection is the process of mapping data points from a high-dimensional space onto a lower-dimensional subspace.\n",
    "\n",
    "ðŸ”¹ In PCA, projection is used to reduce dimensionality by finding the principal components (PCs) and projecting data onto these components.\n",
    "ðŸ”¹ The goal is to preserve as much variance as possible while reducing dimensions.\n",
    "\n",
    "ðŸ“Œ Example in PCA:\n",
    "\n",
    "If we have a 3D dataset, PCA finds the best 2D plane to project the data while keeping the spread (variance) of the data as high as possible.\n",
    "Q2: How Does the Optimization Problem in PCA Work, and What is It Trying to Achieve?\n",
    "PCA solves an optimization problem to find the directions (principal components) that maximize variance while keeping them orthogonal.\n",
    "\n",
    "ðŸ”¹ Objective of PCA:\n",
    "\n",
    "Find a transformation matrix W that maximizes the variance of the projected data.\n",
    "Ensure the new axes (principal components) are orthogonal.\n",
    "ðŸ”¹ Mathematically:\n",
    "\n",
    "PCA finds eigenvectors v of the covariance matrix Î£ that maximize:\n",
    "ð‘£\n",
    "ð‘‡\n",
    "Î£\n",
    "ð‘£\n",
    "v \n",
    "T\n",
    " Î£v\n",
    "Subject to \n",
    "ð‘£\n",
    "ð‘‡\n",
    "ð‘£\n",
    "=\n",
    "1\n",
    "v \n",
    "T\n",
    " v=1 (unit norm constraint).\n",
    "The first principal component captures the most variance, the second PC captures the next most (while being orthogonal to the first), and so on.\n",
    "Q3: What is the Relationship Between Covariance Matrices and PCA?\n",
    "The covariance matrix helps PCA determine the directions of maximum variance.\n",
    "\n",
    "ðŸ”¹ Steps Involved:\n",
    "\n",
    "Compute the covariance matrix (\n",
    "Î£\n",
    "Î£) of the dataset.\n",
    "Î£\n",
    "=\n",
    "1\n",
    "ð‘›\n",
    "ð‘‹\n",
    "ð‘‡\n",
    "ð‘‹\n",
    "Î£= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " X \n",
    "T\n",
    " X\n",
    "Perform eigen decomposition to get eigenvalues and eigenvectors.\n",
    "The eigenvectors define the principal components (directions of maximum variance).\n",
    "The eigenvalues represent the amount of variance captured by each principal component.\n",
    "ðŸ“Œ Intuition: If two features have high covariance, they contain redundant information, so PCA can reduce dimensions by combining them into a single principal component.\n",
    "\n",
    "Q4: How Does the Choice of the Number of Principal Components Impact PCA Performance?\n",
    "Choosing the right number of components is crucial for balancing information retention and dimensionality reduction.\n",
    "\n",
    "ðŸ”¹ Too many components â†’ Less reduction in complexity, possible overfitting.\n",
    "ðŸ”¹ Too few components â†’ Loss of important information, poor model performance.\n",
    "\n",
    "ðŸ“Œ How to choose the number of PCs?\n",
    "\n",
    "Explained Variance Ratio: Choose components that explain 95-99% of the total variance.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(X)\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "optimal_dims = np.argmax(explained_variance >= 0.95) + 1\n",
    "print(\"Optimal dimensions:\", optimal_dims)\n",
    "Scree Plot (Elbow Method): Plot the variance explained vs. number of components and look for an elbow point.\n",
    "Q5: How Can PCA Be Used in Feature Selection, and What Are Its Benefits?\n",
    "ðŸ”¹ Feature Selection with PCA:\n",
    "Instead of using raw features, PCA transforms features into new principal components, and we can use only the most important ones.\n",
    "\n",
    "ðŸ”¹ Benefits:\n",
    "\n",
    "Reduces dimensionality, improving model efficiency.\n",
    "Removes multicollinearity by creating uncorrelated components.\n",
    "Improves generalization by reducing noise in the data.\n",
    "ðŸ“Œ Example:\n",
    "\n",
    "In a dataset with 100 features, PCA may find that only 10 PCs capture 95% of the variance, allowing us to use just those 10 instead of all 100.\n",
    "Q6: Common Applications of PCA in Data Science and Machine Learning\n",
    "ðŸ”¹ Dimensionality Reduction:\n",
    "\n",
    "Used in high-dimensional datasets (e.g., image processing, genetics).\n",
    "ðŸ”¹ Data Visualization:\n",
    "\n",
    "PCA reduces data to 2D or 3D for easy visualization (e.g., clustering analysis).\n",
    "ðŸ”¹ Feature Extraction & Compression:\n",
    "\n",
    "Used in face recognition, text processing, and signal processing.\n",
    "ðŸ”¹ Noise Reduction:\n",
    "\n",
    "PCA removes less informative features, helping models focus on meaningful patterns.\n",
    "ðŸ”¹ Preprocessing for Machine Learning Models:\n",
    "\n",
    "Improves performance of classifiers like Logistic Regression, SVMs.\n",
    "Q7: What is the Relationship Between Spread and Variance in PCA?\n",
    "Spread refers to how widely data points are distributed, while variance quantifies the spread mathematically.\n",
    "\n",
    "ðŸ”¹ PCA captures variance along principal components because higher variance means more information.\n",
    "ðŸ”¹ The first principal component (PC1) captures the direction with the highest spread (variance), the second PC captures the next highest, and so on.\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "\n",
    "If a dataset has more spread along Feature 1 than Feature 2, PCA will assign a higher weight to Feature 1.\n",
    "Q8: How Does PCA Use Spread and Variance to Identify Principal Components?\n",
    "Compute the covariance matrix to find relationships between features.\n",
    "Find eigenvectors and eigenvalues:\n",
    "Eigenvectors = Principal Component Directions\n",
    "Eigenvalues = Variance Captured by Each Component\n",
    "Sort the eigenvalues in descending order and select the top components.\n",
    "Transform data onto these new components.\n",
    "ðŸ“Œ Intuition: PCA looks for directions where data is most spread out (high variance) because they contain the most information.\n",
    "\n",
    "Q9: How Does PCA Handle Data with High Variance in Some Dimensions but Low Variance in Others?\n",
    "ðŸ”¹ PCA prioritizes high-variance dimensions and may discard low-variance dimensions.\n",
    "\n",
    "ðŸ”¹ If one feature has very high variance (e.g., \"salary\" in dollars vs. \"age\" in years), it can dominate PCA.\n",
    "\n",
    "Solution: Standardize data before applying PCA.\n",
    "ðŸ“Œ Example:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardizes each feature to mean 0, variance 1\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
