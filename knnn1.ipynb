{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Difference Between Euclidean and Manhattan Distance in KNN\n",
    "The main difference is how they calculate distances between points:\n",
    "\n",
    "Euclidean Distance (L2 norm): Measures the straight-line (as-the-crow-flies) distance between two points.\n",
    "𝑑\n",
    "𝐸\n",
    "(\n",
    "𝐴\n",
    ",\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝐴\n",
    "𝑖\n",
    "−\n",
    "𝐵\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "d \n",
    "E\n",
    "​\n",
    " (A,B)= \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " (A \n",
    "i\n",
    "​\n",
    " −B \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "Manhattan Distance (L1 norm): Measures the distance by summing the absolute differences along each dimension (like city blocks).\n",
    "𝑑\n",
    "𝑀\n",
    "(\n",
    "𝐴\n",
    ",\n",
    "𝐵\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝐴\n",
    "𝑖\n",
    "−\n",
    "𝐵\n",
    "𝑖\n",
    "∣\n",
    "d \n",
    "M\n",
    "​\n",
    " (A,B)= \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " ∣A \n",
    "i\n",
    "​\n",
    " −B \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Effect on KNN Classifier or Regressor\n",
    "Euclidean Distance: Works well when features are continuous and the data points are distributed evenly.\n",
    "Manhattan Distance: Performs better in high-dimensional or sparse data, where differences in single dimensions are more meaningful.\n",
    "Choosing the right metric can affect performance:\n",
    "\n",
    "If data is dense and features are equally scaled, Euclidean distance is preferable.\n",
    "If data is high-dimensional or contains outliers, Manhattan distance can be more robust.\n",
    "Q2: Choosing the Optimal Value of \n",
    "𝑘\n",
    "k in KNN\n",
    "The choice of \n",
    "𝑘\n",
    "k (number of neighbors) is crucial for performance.\n",
    "\n",
    "Techniques to Determine Optimal \n",
    "𝑘\n",
    "k:\n",
    "Cross-validation:\n",
    "Use k-fold cross-validation to test different \n",
    "𝑘\n",
    "k values and find the one with the best accuracy.\n",
    "Elbow Method:\n",
    "Plot error rate vs. \n",
    "𝑘\n",
    "k, and choose the \n",
    "𝑘\n",
    "k where the error stops decreasing significantly.\n",
    "Grid Search or Random Search:\n",
    "Use GridSearchCV in scikit-learn to systematically test different \n",
    "𝑘\n",
    "k values.\n",
    "Effect of \n",
    "𝑘\n",
    "k on Performance:\n",
    "Small \n",
    "𝑘\n",
    "k (e.g., \n",
    "𝑘\n",
    "=\n",
    "1\n",
    "k=1 or \n",
    "𝑘\n",
    "=\n",
    "3\n",
    "k=3) → More variance, model is sensitive to noise (overfitting).\n",
    "Large \n",
    "𝑘\n",
    "k (e.g., \n",
    "𝑘\n",
    "=\n",
    "20\n",
    "k=20 or more) → More bias, less sensitivity to individual data points (underfitting).\n",
    "Optimal \n",
    "𝑘\n",
    "k is usually found between 3 and 10.\n",
    "\n",
    "Q3: How the Choice of Distance Metric Affects KNN Performance\n",
    "The distance metric determines how similarity is measured, which directly impacts classification or regression.\n",
    "\n",
    "Comparison of Distance Metrics:\n",
    "Metric\tFormula\tBest for\n",
    "Euclidean\t\n",
    "∑\n",
    "(\n",
    "𝐴\n",
    "𝑖\n",
    "−\n",
    "𝐵\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "∑(A \n",
    "i\n",
    "​\n",
    " −B \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \tDense, low-dimensional data\n",
    "Manhattan\t( \\sum\tA_i - B_i\n",
    "Minkowski\t( (\\sum\tA_i - B_i\n",
    "Hamming\tCounts different elements\tCategorical data (text, DNA)\n",
    "When to Choose Each Distance Metric:\n",
    "Euclidean Distance: Best when data is continuous and evenly distributed.\n",
    "Manhattan Distance: Works well for high-dimensional or sparse data.\n",
    "Minkowski Distance: Allows tuning between Euclidean (\n",
    "𝑝\n",
    "=\n",
    "2\n",
    "p=2) and Manhattan (\n",
    "𝑝\n",
    "=\n",
    "1\n",
    "p=1).\n",
    "Hamming Distance: Used for categorical data (e.g., binary text classification).\n",
    "Q4: Common Hyperparameters in KNN and Their Effects\n",
    "Hyperparameter\tEffect on Model\tTuning Strategy\n",
    "𝑘\n",
    "k (Number of neighbors)\tControls bias-variance tradeoff\tCross-validation, elbow method\n",
    "Distance Metric\tAffects how neighbors are measured\tTest multiple (Euclidean, Manhattan, etc.)\n",
    "Weighting Function\tUniform (equal weight) vs. distance-based\tDistance weighting can improve accuracy\n",
    "Algorithm (BallTree, KDTree, brute-force)\tOptimizes search for nearest neighbors\tChoose based on dataset size\n",
    "Tuning Strategies:\n",
    "Grid Search: Use GridSearchCV to test different \n",
    "𝑘\n",
    "k, metrics, and weightings.\n",
    "Random Search: Efficient alternative for large parameter spaces.\n",
    "Feature Scaling: Since KNN is distance-based, normalize data using Min-Max scaling or Standardization.\n",
    "Q5: How Training Set Size Affects KNN Performance\n",
    "Large training sets improve performance (KNN benefits from more examples).\n",
    "Too large datasets slow down predictions (KNN requires computing distances for every new point).\n",
    "Techniques to Optimize Training Set Size:\n",
    "Dimensionality Reduction:\n",
    "Use PCA (Principal Component Analysis) or Feature Selection to reduce feature count.\n",
    "Sampling Techniques:\n",
    "Use stratified sampling to keep data representative while reducing size.\n",
    "Efficient Data Structures:\n",
    "Use KD-Trees or Ball Trees for fast nearest-neighbor search (instead of brute-force).\n",
    "Q6: Drawbacks of KNN & How to Overcome Them\n",
    "Drawback\tProblem\tSolution\n",
    "Computational Complexity\tKNN is slow for large datasets\tUse KD-Trees, Ball Trees, or Approximate Nearest Neighbors (ANN)\n",
    "Curse of Dimensionality\tDistance metrics become less effective in high dimensions\tUse PCA or feature selection\n",
    "Sensitive to Noise\tSmall changes in data affect predictions\tUse larger \n",
    "𝑘\n",
    "k, outlier removal\n",
    "Memory Usage\tKNN stores all data points\tConsider alternative models (e.g., SVM, Decision Trees)\n",
    "Alternatives to KNN When It Fails:\n",
    "Support Vector Machines (SVM) → Works better for high-dimensional data.\n",
    "Decision Trees or Random Forests → Less affected by noise.\n",
    "k-Means Clustering + KNN → Can pre-cluster data before running KNN for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
