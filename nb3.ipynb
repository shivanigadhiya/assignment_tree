{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Probability of an Employee Being a Smoker Given That They Use the Health Insurance Plan\n",
    "We use Bayes' Theorem:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝑆\n",
    "𝑚\n",
    "𝑜\n",
    "𝑘\n",
    "𝑒\n",
    "𝑟\n",
    "∣\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    ")\n",
    "=\n",
    "𝑃\n",
    "(\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    "∣\n",
    "𝑆\n",
    "𝑚\n",
    "𝑜\n",
    "𝑘\n",
    "𝑒\n",
    "𝑟\n",
    ")\n",
    "⋅\n",
    "𝑃\n",
    "(\n",
    "𝑆\n",
    "𝑚\n",
    "𝑜\n",
    "𝑘\n",
    "𝑒\n",
    "𝑟\n",
    ")\n",
    "𝑃\n",
    "(\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    ")\n",
    "P(Smoker∣Insurance)= \n",
    "P(Insurance)\n",
    "P(Insurance∣Smoker)⋅P(Smoker)\n",
    "​\n",
    " \n",
    "From the given data:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    ")\n",
    "=\n",
    "0.7\n",
    "P(Insurance)=0.7 (70% of employees use the insurance)\n",
    "𝑃\n",
    "(\n",
    "𝑆\n",
    "𝑚\n",
    "𝑜\n",
    "𝑘\n",
    "𝑒\n",
    "𝑟\n",
    "∣\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    ")\n",
    "=\n",
    "0.4\n",
    "P(Smoker∣Insurance)=0.4 (40% of those using insurance are smokers)\n",
    "We directly have:\n",
    "\n",
    "𝑃\n",
    "(\n",
    "𝑆\n",
    "𝑚\n",
    "𝑜\n",
    "𝑘\n",
    "𝑒\n",
    "𝑟\n",
    "∣\n",
    "𝐼\n",
    "𝑛\n",
    "𝑠\n",
    "𝑢\n",
    "𝑟\n",
    "𝑎\n",
    "𝑛\n",
    "𝑐\n",
    "𝑒\n",
    ")\n",
    "=\n",
    "0.4\n",
    "P(Smoker∣Insurance)=0.4\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.4 or 40%.\n",
    "\n",
    "Q2: Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "Feature\tBernoulli Naive Bayes\tMultinomial Naive Bayes\n",
    "Type of Data\tBinary (0 or 1)\tDiscrete counts (e.g., word frequency)\n",
    "Usage\tText classification (presence/absence of words)\tText classification (word frequency distribution)\n",
    "Example\tSpam detection (word present or not)\tTopic classification (word count matters)\n",
    "Formula\tUses Bernoulli distribution\tUses multinomial distribution\n",
    "Effect of Zero Values\tPenalizes missing words\tHandles zero frequencies better\n",
    "Q3: How Does Bernoulli Naive Bayes Handle Missing Values?\n",
    "Missing values in Bernoulli Naive Bayes are typically treated as 0 (word absent) rather than truly missing.\n",
    "If a word is missing in an instance, it is assumed not present, which may not always be accurate.\n",
    "To handle missing values effectively, techniques like imputation or Laplace smoothing can be used.\n",
    "Q4: Can Gaussian Naive Bayes Be Used for Multi-Class Classification?\n",
    "Yes, Gaussian Naive Bayes (GNB) supports multi-class classification.\n",
    "\n",
    "It assumes each feature follows a normal (Gaussian) distribution.\n",
    "It calculates the probability for each class and assigns the instance to the most probable class.\n",
    "In scikit-learn, GaussianNB automatically supports multiple classes (not just binary).\n",
    "Example:\n",
    "\n",
    "Medical diagnosis (predicting disease based on continuous symptoms).\n",
    "Iris classification (predicting flower species based on petal/sepal lengths).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)  # Adjust path if needed\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]   # Labels (Spam or Not)\n",
    "\n",
    "# Split into training and testing sets (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit models\n",
    "bnb.fit(X_train, y_train)\n",
    "mnb.fit(X_train, y_train)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Evaluate each model\n",
    "evaluate_model(y_test, y_pred_bnb, \"BernoulliNB\")\n",
    "evaluate_model(y_test, y_pred_mnb, \"MultinomialNB\")\n",
    "evaluate_model(y_test, y_pred_gnb, \"GaussianNB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
