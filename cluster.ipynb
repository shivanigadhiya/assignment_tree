{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Types of Clustering Algorithms and Their Differences\n",
    "Clustering algorithms can be categorized based on their approach and assumptions:\n",
    "\n",
    "Centroid-Based Clustering (Partitioning)\n",
    "\n",
    "Example: K-Means, K-Medoids\n",
    "Approach: Assigns points to clusters based on distance to the cluster centroid.\n",
    "Assumption: Clusters are spherical and evenly sized.\n",
    "Density-Based Clustering\n",
    "\n",
    "Example: DBSCAN, OPTICS\n",
    "Approach: Groups points in high-density regions, ignoring noise/outliers.\n",
    "Assumption: Clusters have varying shapes and densities.\n",
    "Hierarchical Clustering\n",
    "\n",
    "Example: Agglomerative, Divisive Clustering\n",
    "Approach: Builds a tree-like structure (dendrogram) of nested clusters.\n",
    "Assumption: Clusters are formed through a nested structure.\n",
    "Distribution-Based Clustering\n",
    "\n",
    "Example: Gaussian Mixture Models (GMM)\n",
    "Approach: Assumes data is generated from multiple Gaussian distributions.\n",
    "Assumption: Clusters follow a probability distribution.\n",
    "Graph-Based Clustering\n",
    "\n",
    "Example: Spectral Clustering\n",
    "Approach: Uses eigenvalues of graph Laplacians for clustering.\n",
    "Assumption: Clusters can be represented as a graph structure.\n",
    "Q2: What is K-Means Clustering and How Does It Work?\n",
    "K-Means is a centroid-based clustering algorithm that partitions data into \n",
    "𝐾\n",
    "K clusters.\n",
    "\n",
    "Steps:\n",
    "Choose \n",
    "𝐾\n",
    "K random cluster centroids.\n",
    "Assign each point to the nearest centroid (forming clusters).\n",
    "Recalculate centroids as the mean of the assigned points.\n",
    "Repeat steps 2-3 until convergence (centroids no longer change).\n",
    "📌 Formula for Centroid Update:\n",
    "\n",
    "𝐶\n",
    "𝑘\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "𝑘\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑁\n",
    "𝑘\n",
    "𝑥\n",
    "𝑖\n",
    "C \n",
    "k\n",
    "​\n",
    " = \n",
    "N \n",
    "k\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "N \n",
    "k\n",
    "​\n",
    " \n",
    "​\n",
    " x \n",
    "i\n",
    "​\n",
    " \n",
    "where \n",
    "𝐶\n",
    "𝑘\n",
    "C \n",
    "k\n",
    "​\n",
    "  is the new centroid and \n",
    "𝑁\n",
    "𝑘\n",
    "N \n",
    "k\n",
    "​\n",
    "  is the number of points in cluster \n",
    "𝑘\n",
    "k.\n",
    "\n",
    "Q3: Advantages and Limitations of K-Means\n",
    "✅ Advantages:\n",
    "\n",
    "Simple and efficient for large datasets.\n",
    "Works well for spherical clusters.\n",
    "Computationally fast (especially with K-Means++ initialization).\n",
    "❌ Limitations:\n",
    "\n",
    "Sensitive to outliers and noise.\n",
    "Struggles with non-spherical and varying density clusters.\n",
    "Requires predefined \n",
    "𝐾\n",
    "K (can be hard to choose).\n",
    "Comparison with Other Techniques:\n",
    "\n",
    "Clustering Method\tHandles Non-Spherical Clusters?\tHandles Noise?\tScalability\n",
    "K-Means\t❌ No\t❌ No\t✅ High\n",
    "DBSCAN\t✅ Yes\t✅ Yes\t❌ Low\n",
    "Hierarchical\t✅ Yes\t❌ No\t❌ Low\n",
    "Q4: How to Determine the Optimal Number of Clusters in K-Means?\n",
    "Choosing the right \n",
    "𝐾\n",
    "K is crucial. Some common methods:\n",
    "\n",
    "Elbow Method\n",
    "\n",
    "Plot the Within-Cluster Sum of Squares (WCSS) against \n",
    "𝐾\n",
    "K.\n",
    "Choose \n",
    "𝐾\n",
    "K where the WCSS decrease slows (elbow point).\n",
    "𝑊\n",
    "𝐶\n",
    "𝑆\n",
    "𝑆\n",
    "=\n",
    "∑\n",
    "𝑘\n",
    "=\n",
    "1\n",
    "𝐾\n",
    "∑\n",
    "𝑖\n",
    "∈\n",
    "𝐶\n",
    "𝑘\n",
    "∣\n",
    "∣\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝐶\n",
    "𝑘\n",
    "∣\n",
    "∣\n",
    "2\n",
    "WCSS= \n",
    "k=1\n",
    "∑\n",
    "K\n",
    "​\n",
    "  \n",
    "i∈C \n",
    "k\n",
    "​\n",
    " \n",
    "∑\n",
    "​\n",
    " ∣∣x \n",
    "i\n",
    "​\n",
    " −C \n",
    "k\n",
    "​\n",
    " ∣∣ \n",
    "2\n",
    " \n",
    "Silhouette Score\n",
    "\n",
    "Measures how similar a point is to its own cluster vs. other clusters.\n",
    "Range: -1 to 1 (higher is better).\n",
    "Gap Statistic\n",
    "\n",
    "Compares WCSS of K-means with WCSS of random uniform samples.\n",
    "Q5: Applications of K-Means in Real-World Scenarios\n",
    "📌 1. Customer Segmentation\n",
    "\n",
    "E-commerce and marketing use K-Means to group customers based on behavior.\n",
    "📌 2. Image Compression\n",
    "\n",
    "K-Means reduces color variations by clustering pixels into a limited color palette.\n",
    "📌 3. Anomaly Detection\n",
    "\n",
    "Used in fraud detection by identifying unusual transactions.\n",
    "📌 4. Document Clustering\n",
    "\n",
    "Organizing news articles or documents into topics.\n",
    "📌 5. Genetic Research\n",
    "\n",
    "Clustering gene expression data to identify diseases.\n",
    "Q6: How to Interpret the Output of K-Means?\n",
    "Once clustering is done, interpret results by analyzing:\n",
    "\n",
    "🔹 Cluster Centroids:\n",
    "\n",
    "Represent the “typical” point in each cluster.\n",
    "🔹 Cluster Distribution:\n",
    "\n",
    "How many points belong to each cluster? Are they imbalanced?\n",
    "🔹 Feature Importance in Clusters:\n",
    "\n",
    "Which features contribute most to separation?\n",
    "📌 Example: In customer segmentation,\n",
    "\n",
    "Cluster 1: High-spending customers.\n",
    "Cluster 2: Budget-conscious customers.\n",
    "Q7: Challenges in K-Means and How to Overcome Them\n",
    "Choosing K:\n",
    "\n",
    "Use the Elbow Method or Silhouette Score.\n",
    "Sensitivity to Initialization:\n",
    "\n",
    "Use K-Means++ to improve initial centroid selection.\n",
    "Handling Outliers:\n",
    "\n",
    "Remove outliers before clustering or use DBSCAN.\n",
    "Non-Spherical Clusters:\n",
    "\n",
    "Use Gaussian Mixture Models (GMM) or DBSCAN instead.\n",
    "Scalability for Big Data:\n",
    "\n",
    "Use Mini-Batch K-Means for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
