{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Types of Clustering Algorithms and Their Differences\n",
    "Clustering algorithms can be categorized based on their approach and assumptions:\n",
    "\n",
    "Centroid-Based Clustering (Partitioning)\n",
    "\n",
    "Example: K-Means, K-Medoids\n",
    "Approach: Assigns points to clusters based on distance to the cluster centroid.\n",
    "Assumption: Clusters are spherical and evenly sized.\n",
    "Density-Based Clustering\n",
    "\n",
    "Example: DBSCAN, OPTICS\n",
    "Approach: Groups points in high-density regions, ignoring noise/outliers.\n",
    "Assumption: Clusters have varying shapes and densities.\n",
    "Hierarchical Clustering\n",
    "\n",
    "Example: Agglomerative, Divisive Clustering\n",
    "Approach: Builds a tree-like structure (dendrogram) of nested clusters.\n",
    "Assumption: Clusters are formed through a nested structure.\n",
    "Distribution-Based Clustering\n",
    "\n",
    "Example: Gaussian Mixture Models (GMM)\n",
    "Approach: Assumes data is generated from multiple Gaussian distributions.\n",
    "Assumption: Clusters follow a probability distribution.\n",
    "Graph-Based Clustering\n",
    "\n",
    "Example: Spectral Clustering\n",
    "Approach: Uses eigenvalues of graph Laplacians for clustering.\n",
    "Assumption: Clusters can be represented as a graph structure.\n",
    "Q2: What is K-Means Clustering and How Does It Work?\n",
    "K-Means is a centroid-based clustering algorithm that partitions data into \n",
    "ğ¾\n",
    "K clusters.\n",
    "\n",
    "Steps:\n",
    "Choose \n",
    "ğ¾\n",
    "K random cluster centroids.\n",
    "Assign each point to the nearest centroid (forming clusters).\n",
    "Recalculate centroids as the mean of the assigned points.\n",
    "Repeat steps 2-3 until convergence (centroids no longer change).\n",
    "ğŸ“Œ Formula for Centroid Update:\n",
    "\n",
    "ğ¶\n",
    "ğ‘˜\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ‘˜\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ‘˜\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "C \n",
    "k\n",
    "â€‹\n",
    " = \n",
    "N \n",
    "k\n",
    "â€‹\n",
    " \n",
    "1\n",
    "â€‹\n",
    "  \n",
    "i=1\n",
    "âˆ‘\n",
    "N \n",
    "k\n",
    "â€‹\n",
    " \n",
    "â€‹\n",
    " x \n",
    "i\n",
    "â€‹\n",
    " \n",
    "where \n",
    "ğ¶\n",
    "ğ‘˜\n",
    "C \n",
    "k\n",
    "â€‹\n",
    "  is the new centroid and \n",
    "ğ‘\n",
    "ğ‘˜\n",
    "N \n",
    "k\n",
    "â€‹\n",
    "  is the number of points in cluster \n",
    "ğ‘˜\n",
    "k.\n",
    "\n",
    "Q3: Advantages and Limitations of K-Means\n",
    "âœ… Advantages:\n",
    "\n",
    "Simple and efficient for large datasets.\n",
    "Works well for spherical clusters.\n",
    "Computationally fast (especially with K-Means++ initialization).\n",
    "âŒ Limitations:\n",
    "\n",
    "Sensitive to outliers and noise.\n",
    "Struggles with non-spherical and varying density clusters.\n",
    "Requires predefined \n",
    "ğ¾\n",
    "K (can be hard to choose).\n",
    "Comparison with Other Techniques:\n",
    "\n",
    "Clustering Method\tHandles Non-Spherical Clusters?\tHandles Noise?\tScalability\n",
    "K-Means\tâŒ No\tâŒ No\tâœ… High\n",
    "DBSCAN\tâœ… Yes\tâœ… Yes\tâŒ Low\n",
    "Hierarchical\tâœ… Yes\tâŒ No\tâŒ Low\n",
    "Q4: How to Determine the Optimal Number of Clusters in K-Means?\n",
    "Choosing the right \n",
    "ğ¾\n",
    "K is crucial. Some common methods:\n",
    "\n",
    "Elbow Method\n",
    "\n",
    "Plot the Within-Cluster Sum of Squares (WCSS) against \n",
    "ğ¾\n",
    "K.\n",
    "Choose \n",
    "ğ¾\n",
    "K where the WCSS decrease slows (elbow point).\n",
    "ğ‘Š\n",
    "ğ¶\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "=\n",
    "âˆ‘\n",
    "ğ‘˜\n",
    "=\n",
    "1\n",
    "ğ¾\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "âˆˆ\n",
    "ğ¶\n",
    "ğ‘˜\n",
    "âˆ£\n",
    "âˆ£\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ¶\n",
    "ğ‘˜\n",
    "âˆ£\n",
    "âˆ£\n",
    "2\n",
    "WCSS= \n",
    "k=1\n",
    "âˆ‘\n",
    "K\n",
    "â€‹\n",
    "  \n",
    "iâˆˆC \n",
    "k\n",
    "â€‹\n",
    " \n",
    "âˆ‘\n",
    "â€‹\n",
    " âˆ£âˆ£x \n",
    "i\n",
    "â€‹\n",
    " âˆ’C \n",
    "k\n",
    "â€‹\n",
    " âˆ£âˆ£ \n",
    "2\n",
    " \n",
    "Silhouette Score\n",
    "\n",
    "Measures how similar a point is to its own cluster vs. other clusters.\n",
    "Range: -1 to 1 (higher is better).\n",
    "Gap Statistic\n",
    "\n",
    "Compares WCSS of K-means with WCSS of random uniform samples.\n",
    "Q5: Applications of K-Means in Real-World Scenarios\n",
    "ğŸ“Œ 1. Customer Segmentation\n",
    "\n",
    "E-commerce and marketing use K-Means to group customers based on behavior.\n",
    "ğŸ“Œ 2. Image Compression\n",
    "\n",
    "K-Means reduces color variations by clustering pixels into a limited color palette.\n",
    "ğŸ“Œ 3. Anomaly Detection\n",
    "\n",
    "Used in fraud detection by identifying unusual transactions.\n",
    "ğŸ“Œ 4. Document Clustering\n",
    "\n",
    "Organizing news articles or documents into topics.\n",
    "ğŸ“Œ 5. Genetic Research\n",
    "\n",
    "Clustering gene expression data to identify diseases.\n",
    "Q6: How to Interpret the Output of K-Means?\n",
    "Once clustering is done, interpret results by analyzing:\n",
    "\n",
    "ğŸ”¹ Cluster Centroids:\n",
    "\n",
    "Represent the â€œtypicalâ€ point in each cluster.\n",
    "ğŸ”¹ Cluster Distribution:\n",
    "\n",
    "How many points belong to each cluster? Are they imbalanced?\n",
    "ğŸ”¹ Feature Importance in Clusters:\n",
    "\n",
    "Which features contribute most to separation?\n",
    "ğŸ“Œ Example: In customer segmentation,\n",
    "\n",
    "Cluster 1: High-spending customers.\n",
    "Cluster 2: Budget-conscious customers.\n",
    "Q7: Challenges in K-Means and How to Overcome Them\n",
    "Choosing K:\n",
    "\n",
    "Use the Elbow Method or Silhouette Score.\n",
    "Sensitivity to Initialization:\n",
    "\n",
    "Use K-Means++ to improve initial centroid selection.\n",
    "Handling Outliers:\n",
    "\n",
    "Remove outliers before clustering or use DBSCAN.\n",
    "Non-Spherical Clusters:\n",
    "\n",
    "Use Gaussian Mixture Models (GMM) or DBSCAN instead.\n",
    "Scalability for Big Data:\n",
    "\n",
    "Use Mini-Batch K-Means for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
